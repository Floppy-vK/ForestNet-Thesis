{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Purposes\n",
    "This notebook is used to convert the existing dataset images to numpy arrays, pairing them up with other images to increase input complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new CSV files which only include examples that contain cloud filtered images along with composite images\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#read through all files in training, val, and test sets\n",
    "train = pd.read_csv('../input/forestnet/ForestNetDataset/train.csv')\n",
    "val = pd.read_csv('../input/forestnet/ForestNetDataset/val.csv')\n",
    "test = pd.read_csv('../input/forestnet/ForestNetDataset/test.csv')\n",
    "\n",
    "train_dict = {'label':[], 'merged_label': [], 'latitude': [], 'longitude': [], 'year': [], 'example_path': []}\n",
    "val_dict = {'label':[], 'merged_label': [], 'latitude': [], 'longitude': [], 'year': [], 'example_path': []}\n",
    "test_dict = {'label':[], 'merged_label': [], 'latitude': [], 'longitude': [], 'year': [], 'example_path': []}\n",
    "\n",
    "\n",
    "#check to see if cloud filtered image is available\n",
    "for example_set_df, set_dict, set_path in zip([train, val, test], [train_dict, val_dict, test_dict], ['train_new.csv', 'val_new.csv', 'test_new.csv']):   \n",
    "    for i in range(example_set_df.shape[0]):\n",
    "            img_path = f'../input/forestnet/ForestNetDataset/{example_set_df[\"example_path\"][i]}/images/visible'\n",
    "            list_of_images = glob.glob(img_path + '/*.png')\n",
    "            if len(list_of_images) > 1:\n",
    "                  for col in ['label', 'merged_label', 'latitude', 'longitude', 'year', 'example_path']:\n",
    "                    set_dict[col].append(example_set_df[col][i])\n",
    "    set_df = pd.DataFrame(set_dict)\n",
    "    set_df.to_csv('../input/forestnet/ForestNetDataset/' + set_path, index=False)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO ONLY INCLUDE EXAMPLES WITH COMPOSITE AND CLOUD FILTERED IMAGES using np.concatenate\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#path to set csv files\n",
    "train = pd.read_csv('../input/forestnet/ForestNetDataset/train_new.csv')\n",
    "val = pd.read_csv('../input/forestnet/ForestNetDataset/val_new.csv')\n",
    "test = pd.read_csv('../input/forestnet/ForestNetDataset/test_new.csv')\n",
    "\n",
    "for example_set_df in [train, val, test]:\n",
    "    for i in range(example_set_df.shape[0]):\n",
    "        img_path = f'../input/forestnet/ForestNetDataset/{example_set_df[\"example_path\"][i]}/images/visible'\n",
    "        #retrieve all image paths for example\n",
    "        list_of_images = glob.glob(img_path + \"/*.png\")\n",
    "        #loads example's composite image\n",
    "        composite_img = Image.open(list_of_images[-1])\n",
    "        #loads most recent cloud filtered image\n",
    "        cloudf_img = Image.open(list_of_images[-2])\n",
    "        #convert to np-array\n",
    "        np_composite_img = np.asarray(composite_img)\n",
    "        np_cloudf_img = np.asarray(cloudf_img)\n",
    "        # COULD ALSO USE DIFFERENT AXIS TO GET NON-SQUARE IMAGE, I DON'T THINK THAT'S A GOOD IDEA\n",
    "        np_new = np.concatenate((np_composite_img, np_cloudf_img), 2)\n",
    "        np.save(img_path + \"/concatenated_images.npy\", np_new)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO ONLY INCLUDE EXAMPLES WITH COMPOSITE AND CLOUD FILTERED IMAGES USING PIL.Image.blend\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#path to set csv files\n",
    "train = pd.read_csv('../input/forestnet/ForestNetDataset/train_new.csv')\n",
    "val = pd.read_csv('../input/forestnet/ForestNetDataset/val_new.csv')\n",
    "test = pd.read_csv('../input/forestnet/ForestNetDataset/test_new.csv')\n",
    "\n",
    "for example_set_df in [train, val, test]:\n",
    "    for i in range(example_set_df.shape[0]):\n",
    "        img_path = f'../input/forestnet/ForestNetDataset/{example_set_df[\"example_path\"][i]}/images/visible'\n",
    "        #retrieve all image paths for example\n",
    "        list_of_images = glob.glob(img_path + \"/*.png\")\n",
    "        #loads example's composite image\n",
    "        composite_img = Image.open(list_of_images[-1])\n",
    "        #loads most recent cloud filtered image\n",
    "        cloudf_img = Image.open(list_of_images[-2])\n",
    "        #blend images\n",
    "        blended_images = Image.blend(composite_img, cloudf_img, 0.5)\n",
    "        blended_images.save(img_path + \"/blended_images.png\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0667, 0.0627, 0.0627,  ..., 0.1098, 0.0980, 0.0863],\n",
      "         [0.0667, 0.0667, 0.0627,  ..., 0.1686, 0.1725, 0.1686],\n",
      "         [0.0667, 0.0627, 0.0627,  ..., 0.1608, 0.1569, 0.1961],\n",
      "         ...,\n",
      "         [0.0863, 0.0863, 0.0784,  ..., 0.1412, 0.1373, 0.1294],\n",
      "         [0.0784, 0.0863, 0.0902,  ..., 0.1333, 0.1333, 0.1294],\n",
      "         [0.0784, 0.0863, 0.0902,  ..., 0.1412, 0.1373, 0.1373]],\n",
      "\n",
      "        [[0.0941, 0.0941, 0.0902,  ..., 0.1373, 0.1216, 0.1137],\n",
      "         [0.0941, 0.0941, 0.0941,  ..., 0.1961, 0.2000, 0.2078],\n",
      "         [0.0941, 0.0980, 0.0941,  ..., 0.1765, 0.1765, 0.2118],\n",
      "         ...,\n",
      "         [0.1373, 0.1373, 0.1255,  ..., 0.1373, 0.1373, 0.1412],\n",
      "         [0.1294, 0.1333, 0.1373,  ..., 0.1294, 0.1373, 0.1412],\n",
      "         [0.1294, 0.1333, 0.1373,  ..., 0.1490, 0.1451, 0.1451]],\n",
      "\n",
      "        [[0.0510, 0.0471, 0.0510,  ..., 0.0784, 0.0510, 0.0510],\n",
      "         [0.0510, 0.0510, 0.0471,  ..., 0.1373, 0.1373, 0.1451],\n",
      "         [0.0510, 0.0510, 0.0510,  ..., 0.1569, 0.1412, 0.1529],\n",
      "         ...,\n",
      "         [0.0745, 0.0784, 0.0667,  ..., 0.0863, 0.0863, 0.0863],\n",
      "         [0.0706, 0.0784, 0.0784,  ..., 0.0745, 0.0824, 0.0784],\n",
      "         [0.0706, 0.0745, 0.0824,  ..., 0.0902, 0.0863, 0.0824]],\n",
      "\n",
      "        [[0.0314, 0.0235, 0.0196,  ..., 0.0235, 0.0196, 0.0157],\n",
      "         [0.0353, 0.0196, 0.0196,  ..., 0.0667, 0.1137, 0.0902],\n",
      "         [0.0314, 0.0235, 0.0196,  ..., 0.1216, 0.0902, 0.0784],\n",
      "         ...,\n",
      "         [0.2118, 0.2314, 0.2706,  ..., 0.0902, 0.0863, 0.0863],\n",
      "         [0.2510, 0.2863, 0.2627,  ..., 0.0902, 0.0745, 0.0863],\n",
      "         [0.2824, 0.2902, 0.2784,  ..., 0.1294, 0.0902, 0.1020]],\n",
      "\n",
      "        [[0.0627, 0.0471, 0.0471,  ..., 0.0471, 0.0431, 0.0392],\n",
      "         [0.0706, 0.0510, 0.0471,  ..., 0.0902, 0.1412, 0.1176],\n",
      "         [0.0667, 0.0549, 0.0510,  ..., 0.1451, 0.1098, 0.1020],\n",
      "         ...,\n",
      "         [0.1686, 0.1882, 0.2235,  ..., 0.1059, 0.0980, 0.0980],\n",
      "         [0.2000, 0.2314, 0.2039,  ..., 0.1020, 0.0824, 0.1020],\n",
      "         [0.2275, 0.2275, 0.2157,  ..., 0.1490, 0.0980, 0.1059]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0039, 0.0000, 0.0000,  ..., 0.0235, 0.0784, 0.0549],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0824, 0.0471, 0.0392],\n",
      "         ...,\n",
      "         [0.0824, 0.1020, 0.1412,  ..., 0.0196, 0.0157, 0.0196],\n",
      "         [0.1176, 0.1569, 0.1255,  ..., 0.0157, 0.0000, 0.0196],\n",
      "         [0.1451, 0.1490, 0.1373,  ..., 0.0588, 0.0118, 0.0235]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms \n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose([ \n",
    "    transforms.ToTensor() \n",
    "]) \n",
    "\n",
    "image = np.load('../input/forestnet/ForestNetDataset/examples/0.01805967191228_101.57018668487896/images/visible/concatenated_images.npy')\n",
    "# Convert the image to Torch tensor \n",
    "tensor = transform(image) \n",
    "  \n",
    "# print the converted image tensor \n",
    "print(tensor) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
